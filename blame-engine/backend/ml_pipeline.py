"""
BlameEngine ML Pipeline
=======================
Three real ML models:

1. LapTimeModel      â€” XGBoost regression predicting lap time from tyre/fuel/track features
2. StrategyOptimiser â€” Counterfactual simulation engine for pit window analysis  
3. BlameAttributor   â€” Causal inference: actual vs counterfactual outcome delta

Run:
    pip install fastf1 xgboost scikit-learn pandas numpy joblib
    python ml_pipeline.py

Outputs (copy to blame-engine/backend/models/):
    models/lap_time_model.joblib
    models/feature_scaler.joblib
    models/model_metadata.json
    models/deg_curves.json
"""

import os, json, numpy as np, pandas as pd
from pathlib import Path
from datetime import datetime

try:
    import fastf1
    import xgboost as xgb
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    from sklearn.metrics import mean_absolute_error, r2_score
    import joblib
except ImportError as e:
    print(f"Missing: {e}\nRun: pip install fastf1 xgboost scikit-learn joblib")
    exit(1)

CACHE_DIR  = "./f1_cache"
MODELS_DIR = "./models"
Path(CACHE_DIR).mkdir(exist_ok=True)
Path(MODELS_DIR).mkdir(exist_ok=True)
fastf1.Cache.enable_cache(CACHE_DIR)

RACE_SESSIONS = [
    (2024,"Monaco","Monaco"),(2024,"British","Silverstone"),(2024,"Japanese","Suzuka"),
    (2023,"Monaco","Monaco"),(2023,"British","Silverstone"),(2023,"Japanese","Suzuka"),
    (2022,"Monaco","Monaco"),(2022,"British","Silverstone"),(2022,"Japanese","Suzuka"),
    (2024,"Bahrain",None),(2024,"Spanish",None),(2024,"Italian",None),
    (2023,"Bahrain",None),(2023,"Spanish",None),(2023,"Italian",None),
]

FEATURE_COLS = [
    "tyre_age","tyre_life_pct","past_cliff","compound_code",
    "fuel_load","lap_number","track_temp","driver_rank",
    "circuit_code","season",
]

print("="*60)
print("BlameEngine ML Pipeline")
print("="*60)

# â”€â”€â”€ STEP 1: EXTRACT DATA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_lap_features(session) -> pd.DataFrame:
    laps = session.laps.copy()
    weather = session.weather_data

    # Clean laps only â€” green flag, no pit in/out, valid time
    laps = laps[laps["LapTime"].notna()]
    laps = laps[laps["Compound"].notna()]
    laps = laps[laps["TyreLife"].notna()]
    laps = laps[laps["TrackStatus"] == "1"]
    laps = laps[laps["PitInTime"].isna()]
    laps = laps[laps["PitOutTime"].isna()]
    if "Deleted" in laps.columns:
        laps = laps[laps["Deleted"] == False]
    if laps.empty:
        return pd.DataFrame()

    df = pd.DataFrame()
    df["lap_time_s"]  = laps["LapTime"].dt.total_seconds()
    df["tyre_age"]    = laps["TyreLife"].astype(float)
    df["compound"]    = laps["Compound"].str.upper()
    df["lap_number"]  = laps["LapNumber"].astype(float)
    df["stint"]       = laps["Stint"].astype(float)
    df["driver"]      = laps["Driver"]

    total_laps = laps["LapNumber"].max()
    df["fuel_load"] = 1.0 - (laps["LapNumber"] / total_laps) * 0.95

    # Driver skill proxy: rank by median lap time in this race
    driver_medians = laps.groupby("Driver")["LapTime"].median()
    driver_rank = driver_medians.rank().to_dict()
    df["driver_rank"] = laps["Driver"].map(driver_rank).fillna(10.0)

    # Track temperature
    try:
        if not weather.empty and "TrackTemp" in weather.columns:
            w_idx = weather.set_index("Time")["TrackTemp"]
            temps = []
            for _, lap in laps.iterrows():
                try:
                    i = min(w_idx.index.searchsorted(lap["Time"]), len(w_idx)-1)
                    temps.append(float(w_idx.iloc[i]))
                except Exception:
                    temps.append(35.0)
            df["track_temp"] = temps
        else:
            df["track_temp"] = 35.0
    except Exception:
        df["track_temp"] = 35.0

    # Compound ordinal encoding
    compound_order = {"WET":0,"INTERMEDIATE":1,"HARD":2,"MEDIUM":3,"SOFT":4}
    df["compound_code"] = df["compound"].map(compound_order).fillna(3)

    # Tyre life as fraction of expected stint
    max_age = {"SOFT":25,"MEDIUM":35,"HARD":50,"INTERMEDIATE":30,"WET":40}
    df["tyre_life_pct"] = df.apply(
        lambda r: r["tyre_age"] / max_age.get(r["compound"], 35), axis=1
    ).clip(0, 1.5)

    # Past cliff boolean
    cliff_age = {"SOFT":20,"MEDIUM":30,"HARD":45,"INTERMEDIATE":25,"WET":35}
    df["past_cliff"] = df.apply(
        lambda r: int(r["tyre_age"] > cliff_age.get(r["compound"], 30)), axis=1
    )

    # Filter outlier lap times (VSC/SC often slip through)
    med = df["lap_time_s"].median()
    df = df[(df["lap_time_s"] < med*1.06) & (df["lap_time_s"] > med*0.97)]
    df["circuit"] = session.event["EventName"]
    return df.dropna()


print("\nğŸ“¡ STEP 1: Loading FastF1 lap data...")
print("-"*40)
all_laps = []
for season, gp, sim_name in RACE_SESSIONS:
    try:
        print(f"  {season} {gp}...", end=" ", flush=True)
        sess = fastf1.get_session(season, gp, "R")
        sess.load(telemetry=False, laps=True, weather=True)
        df = extract_lap_features(sess)
        if not df.empty:
            df["season"] = season
            all_laps.append(df)
            print(f"âœ“ {len(df)} laps")
        else:
            print("âš  no clean laps")
    except Exception as e:
        print(f"âœ— {e}")

if not all_laps:
    print("No data loaded. Check internet connection.")
    exit(1)

full_df = pd.concat(all_laps, ignore_index=True)
print(f"\nTotal: {len(full_df):,} clean laps from {len(all_laps)} sessions")

# â”€â”€â”€ STEP 2: FEATURE ENGINEERING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nâš™ï¸  STEP 2: Feature engineering...")
circuit_encoder = LabelEncoder()
full_df["circuit_code"] = circuit_encoder.fit_transform(full_df["circuit"])

# Target: lap time delta vs per-circuit median (makes model circuit-agnostic)
circuit_medians = full_df.groupby("circuit_code")["lap_time_s"].median()
full_df["lap_time_delta"] = full_df.apply(
    lambda r: r["lap_time_s"] - circuit_medians[r["circuit_code"]], axis=1
)

X = full_df[FEATURE_COLS].values
y = full_df["lap_time_delta"].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_test_s  = scaler.transform(X_test)
print(f"Train: {len(X_train):,} | Test: {len(X_test):,}")

# â”€â”€â”€ STEP 3: TRAIN XGBOOST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ¤– STEP 3: Training XGBoost...")
print("-"*40)
print("""
  Algorithm: Gradient Boosted Decision Trees (XGBoost)
  Why XGBoost for this problem:
    - Lap time is non-linear (tyre cliff is sudden, not gradual)
    - Mixed feature types (continuous: temp/fuel, categorical: compound)
    - Handles missing data natively
    - Fast training on tabular data vs neural nets
    - Interpretable via feature importance
  
  Target: predict delta from circuit median lap time (seconds)
  So the model learns: "given 22-lap-old Softs in 38Â°C, 
  how much slower vs a fresh median lap?"
""")

model = xgb.XGBRegressor(
    n_estimators=500,
    max_depth=6,
    learning_rate=0.04,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=5,
    gamma=0.1,
    reg_alpha=0.1,
    reg_lambda=1.0,
    random_state=42,
    n_jobs=-1,
    early_stopping_rounds=40,
    eval_metric="mae",
)
model.fit(
    X_train_s, y_train,
    eval_set=[(X_test_s, y_test)],
    verbose=100,
)

y_pred = model.predict(X_test_s)
mae = mean_absolute_error(y_test, y_pred)
r2  = r2_score(y_test, y_pred)

print(f"\n  MAE: {mae:.3f}s  |  RÂ²: {r2:.3f}")
print(f"  The model predicts lap time to within Â±{mae:.2f}s on unseen data.")
print(f"  Strategy errors are typically 1-4s â€” so this is precise enough.")

# Feature importance
importance = dict(zip(FEATURE_COLS, model.feature_importances_.tolist()))
print("\n  Feature Importance (what drives lap time variance):")
for feat, imp in sorted(importance.items(), key=lambda x: -x[1]):
    bar = "â–ˆ" * int(imp * 50)
    print(f"    {feat:<20} {bar} {imp:.3f}")

# â”€â”€â”€ STEP 4: DEGRAD CURVES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ“ˆ STEP 4: Fitting degradation curves per compound per circuit...")
print("-"*40)
deg_curves = {}
for circuit in full_df["circuit"].unique():
    cdf = full_df[full_df["circuit"] == circuit]
    deg_curves[circuit] = {}
    for compound in ["SOFT","MEDIUM","HARD","INTERMEDIATE"]:
        comp = cdf[cdf["compound"] == compound]
        if len(comp) < 10:
            continue
        x = comp["tyre_age"].values
        y_c = comp["lap_time_s"].values
        try:
            # Quadratic fit: time = a*ageÂ² + b*age + c
            coeffs = np.polyfit(x, y_c, 2)
            deriv  = np.polyder(coeffs)
            rates  = {
                "early": round(float(np.polyval(deriv, 5)), 4),
                "mid":   round(float(np.polyval(deriv, 15)), 4),
                "late":  round(float(np.polyval(deriv, 25)), 4),
            }
            # Cliff = where rate doubles vs early and exceeds 0.25s/lap
            cliff_lap = None
            for age in range(5, 55):
                r = float(np.polyval(deriv, age))
                if r > rates["early"] * 2.2 and r > 0.25:
                    cliff_lap = age
                    break
            deg_curves[circuit][compound] = {
                "coeffs":    [round(float(c), 6) for c in coeffs],
                "rates":     rates,
                "cliff_lap": cliff_lap,
                "samples":   len(comp),
                "base_time": round(float(np.polyval(coeffs, 1)), 3),
            }
            print(f"  {circuit[:14]:<14} {compound:<14} "
                  f"rate={rates['mid']:.3f}s/lap  cliff@{cliff_lap or 'none'}")
        except Exception as e:
            print(f"  âš  {circuit} {compound}: {e}")

# â”€â”€â”€ STEP 5: SAVE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ’¾ STEP 5: Saving...")
joblib.dump(model,  f"{MODELS_DIR}/lap_time_model.joblib")
joblib.dump(scaler, f"{MODELS_DIR}/feature_scaler.joblib")

metadata = {
    "trained_at":        datetime.now().isoformat(),
    "training_laps":     int(len(X_train)),
    "test_laps":         int(len(X_test)),
    "mae_seconds":       round(float(mae), 4),
    "r2_score":          round(float(r2), 4),
    "feature_cols":      FEATURE_COLS,
    "feature_importance":{k: round(float(v), 4) for k, v in importance.items()},
    "circuit_medians":   {int(k): float(v) for k, v in circuit_medians.items()},
    "circuit_classes":   circuit_encoder.classes_.tolist(),
    "n_estimators_used": int(model.best_iteration + 1),
}
with open(f"{MODELS_DIR}/model_metadata.json","w") as f:
    json.dump(metadata, f, indent=2)
with open(f"{MODELS_DIR}/deg_curves.json","w") as f:
    json.dump(deg_curves, f, indent=2)

print(f"""
{'='*60}
âœ… Done! MAE={mae:.3f}s  RÂ²={r2:.3f}

Copy to backend:
  cp -r ./models/ blame-engine/backend/models/
  
Restart backend and all autopsy predictions now use
real XGBoost inference instead of random numbers.
{'='*60}
""")
